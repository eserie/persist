{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import pytest\n",
    "from persist.persist import PersistentDAG\n",
    "from functools import wraps\n",
    "\n",
    "# global variable to simulate the fact to have serialize data somewhere\n",
    "IS_COMPUTED = dict()\n",
    "\n",
    "from time import sleep\n",
    "def load_data(**kwargs):\n",
    "    sleep(2)\n",
    "    print 'load data ...'\n",
    "    if kwargs:\n",
    "        print kwargs\n",
    "        return 'data_{}'.format(kwargs)\n",
    "    return 'data'\n",
    "\n",
    "\n",
    "def clean_data(data):\n",
    "    assert isinstance(data, str)\n",
    "    print 'clean data ...'\n",
    "    return 'cleaned_{}'.format(data)\n",
    "\n",
    "\n",
    "def analyze_data(cleaned_data, option=1, **other_options):\n",
    "    assert isinstance(cleaned_data, str)\n",
    "    print 'analyze data ...'\n",
    "    return 'analyzed_{}'.format(cleaned_data)\n",
    "\n",
    "\n",
    "\n",
    "class Serializer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load(self, key):\n",
    "        print \"load data for key {} ...\".format(key)\n",
    "        return IS_COMPUTED[key]\n",
    "\n",
    "    def dump(self, key, value):\n",
    "        print \"save data with key {} ...\".format(key)\n",
    "        IS_COMPUTED[key] = value\n",
    "\n",
    "    def is_computed(self, key):\n",
    "        return IS_COMPUTED.get(key) is not None\n",
    "\n",
    "    def delayed_load(self, key):\n",
    "        def load():\n",
    "            return self.load(key)\n",
    "        return load\n",
    "\n",
    "    def dump_result(self, func, key):\n",
    "        @wraps(func)\n",
    "        def wrapped_func(*args, **kwargs):\n",
    "            result = func(*args, **kwargs)\n",
    "            self.dump(key, result)\n",
    "            return result\n",
    "        return wrapped_func\n",
    "\n",
    "\n",
    "def setup_graph(**kwargs):\n",
    "    g = PersistentDAG(**kwargs)\n",
    "    serializer = Serializer()\n",
    "    for pool in ['pool1', 'pool2']:\n",
    "        g.add_task(('data', pool), serializer, load_data)\n",
    "        g.add_task(('cleaned_data', pool), serializer,\n",
    "                   clean_data, ('data', pool))\n",
    "        g.add_task(('analyzed_data', pool), serializer,\n",
    "                   analyze_data, ('cleaned_data', pool))\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "global IS_COMPUTED\n",
    "IS_COMPUTED = dict()\n",
    "g = setup_graph(use_cluster=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"621pt\" viewBox=\"0.00 0.00 351.92 620.68\" width=\"352pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 616.678)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-616.678 347.923,-616.678 347.923,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- &#45;5300776722108939485 -->\n",
       "<g class=\"node\" id=\"node1\"><title>-5300776722108939485</title>\n",
       "<polygon fill=\"none\" points=\"159.385,-381.801 3.53848,-381.801 3.53848,-345.801 159.385,-345.801 159.385,-381.801\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.4615\" y=\"-359.601\">('cleaned_data', 'pool2')</text>\n",
       "</g>\n",
       "<!-- 2410200536988902493 -->\n",
       "<g class=\"node\" id=\"node5\"><title>2410200536988902493</title>\n",
       "<ellipse cx=\"81.4615\" cy=\"-479.239\" fill=\"none\" rx=\"61.3761\" ry=\"61.3761\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.4615\" y=\"-475.039\">analyze_data</text>\n",
       "</g>\n",
       "<!-- &#45;5300776722108939485&#45;&gt;2410200536988902493 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>-5300776722108939485-&gt;2410200536988902493</title>\n",
       "<path d=\"M81.4615,-382.03C81.4615,-389.252 81.4615,-398.17 81.4615,-407.638\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.9616,-407.656 81.4615,-417.656 84.9616,-407.656 77.9616,-407.656\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 2538146433059717352 -->\n",
       "<g class=\"node\" id=\"node2\"><title>2538146433059717352</title>\n",
       "<ellipse cx=\"81.4615\" cy=\"-257.479\" fill=\"none\" rx=\"52.1463\" ry=\"52.1463\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.4615\" y=\"-253.279\">clean_data</text>\n",
       "</g>\n",
       "<!-- 2538146433059717352&#45;&gt;&#45;5300776722108939485 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>2538146433059717352-&gt;-5300776722108939485</title>\n",
       "<path d=\"M81.4615,-310.029C81.4615,-318.682 81.4615,-327.357 81.4615,-335.102\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.9616,-335.333 81.4615,-345.333 84.9616,-335.333 77.9616,-335.333\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5200419663265502364 -->\n",
       "<g class=\"node\" id=\"node3\"><title>5200419663265502364</title>\n",
       "<polygon fill=\"none\" points=\"134.636,-169.156 28.2874,-169.156 28.2874,-133.156 134.636,-133.156 134.636,-169.156\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.4615\" y=\"-146.956\">('data', 'pool2')</text>\n",
       "</g>\n",
       "<!-- 5200419663265502364&#45;&gt;2538146433059717352 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>5200419663265502364-&gt;2538146433059717352</title>\n",
       "<path d=\"M81.4615,-169.383C81.4615,-176.608 81.4615,-185.487 81.4615,-194.786\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.9616,-194.981 81.4615,-204.981 84.9616,-194.981 77.9616,-194.981\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- &#45;1443455424474306218 -->\n",
       "<g class=\"node\" id=\"node4\"><title>-1443455424474306218</title>\n",
       "<polygon fill=\"none\" points=\"162.885,-612.678 0.0384818,-612.678 0.0384818,-576.678 162.885,-576.678 162.885,-612.678\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.4615\" y=\"-590.478\">('analyzed_data', 'pool2')</text>\n",
       "</g>\n",
       "<!-- 2410200536988902493&#45;&gt;&#45;1443455424474306218 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>2410200536988902493-&gt;-1443455424474306218</title>\n",
       "<path d=\"M81.4615,-540.944C81.4615,-549.823 81.4615,-558.572 81.4615,-566.313\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.9616,-566.495 81.4615,-576.495 84.9616,-566.495 77.9616,-566.495\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 5200419663266584889 -->\n",
       "<g class=\"node\" id=\"node6\"><title>5200419663266584889</title>\n",
       "<polygon fill=\"none\" points=\"315.636,-169.156 209.287,-169.156 209.287,-133.156 315.636,-133.156 315.636,-169.156\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.462\" y=\"-146.956\">('data', 'pool1')</text>\n",
       "</g>\n",
       "<!-- 8835412057947564777 -->\n",
       "<g class=\"node\" id=\"node9\"><title>8835412057947564777</title>\n",
       "<ellipse cx=\"262.462\" cy=\"-257.479\" fill=\"none\" rx=\"52.1463\" ry=\"52.1463\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.462\" y=\"-253.279\">clean_data</text>\n",
       "</g>\n",
       "<!-- 5200419663266584889&#45;&gt;8835412057947564777 -->\n",
       "<g class=\"edge\" id=\"edge7\"><title>5200419663266584889-&gt;8835412057947564777</title>\n",
       "<path d=\"M262.462,-169.383C262.462,-176.608 262.462,-185.487 262.462,-194.786\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"258.962,-194.981 262.462,-204.981 265.962,-194.981 258.962,-194.981\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1467703301102581010 -->\n",
       "<g class=\"node\" id=\"node7\"><title>1467703301102581010</title>\n",
       "<ellipse cx=\"262.462\" cy=\"-48.5779\" fill=\"none\" rx=\"48.6559\" ry=\"48.6559\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.462\" y=\"-44.3779\">load_data</text>\n",
       "</g>\n",
       "<!-- 1467703301102581010&#45;&gt;5200419663266584889 -->\n",
       "<g class=\"edge\" id=\"edge5\"><title>1467703301102581010-&gt;5200419663266584889</title>\n",
       "<path d=\"M262.462,-97.216C262.462,-106.013 262.462,-114.918 262.462,-122.851\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"258.962,-122.964 262.462,-132.964 265.962,-122.964 258.962,-122.964\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- &#45;5300776722105691910 -->\n",
       "<g class=\"node\" id=\"node8\"><title>-5300776722105691910</title>\n",
       "<polygon fill=\"none\" points=\"340.385,-381.801 184.538,-381.801 184.538,-345.801 340.385,-345.801 340.385,-381.801\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.462\" y=\"-359.601\">('cleaned_data', 'pool1')</text>\n",
       "</g>\n",
       "<!-- 4308161845663928668 -->\n",
       "<g class=\"node\" id=\"node12\"><title>4308161845663928668</title>\n",
       "<ellipse cx=\"262.462\" cy=\"-479.239\" fill=\"none\" rx=\"61.3761\" ry=\"61.3761\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.462\" y=\"-475.039\">analyze_data</text>\n",
       "</g>\n",
       "<!-- &#45;5300776722105691910&#45;&gt;4308161845663928668 -->\n",
       "<g class=\"edge\" id=\"edge10\"><title>-5300776722105691910-&gt;4308161845663928668</title>\n",
       "<path d=\"M262.462,-382.03C262.462,-389.252 262.462,-398.17 262.462,-407.638\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"258.962,-407.656 262.462,-417.656 265.962,-407.656 258.962,-407.656\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 8835412057947564777&#45;&gt;&#45;5300776722105691910 -->\n",
       "<g class=\"edge\" id=\"edge6\"><title>8835412057947564777-&gt;-5300776722105691910</title>\n",
       "<path d=\"M262.462,-310.029C262.462,-318.682 262.462,-327.357 262.462,-335.102\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"258.962,-335.333 262.462,-345.333 265.962,-335.333 258.962,-335.333\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 1229595320426462135 -->\n",
       "<g class=\"node\" id=\"node10\"><title>1229595320426462135</title>\n",
       "<ellipse cx=\"81.4615\" cy=\"-48.5779\" fill=\"none\" rx=\"48.6559\" ry=\"48.6559\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.4615\" y=\"-44.3779\">load_data</text>\n",
       "</g>\n",
       "<!-- 1229595320426462135&#45;&gt;5200419663265502364 -->\n",
       "<g class=\"edge\" id=\"edge8\"><title>1229595320426462135-&gt;5200419663265502364</title>\n",
       "<path d=\"M81.4615,-97.216C81.4615,-106.013 81.4615,-114.918 81.4615,-122.851\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"77.9616,-122.964 81.4615,-132.964 84.9616,-122.964 77.9616,-122.964\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- &#45;1443455424477553793 -->\n",
       "<g class=\"node\" id=\"node11\"><title>-1443455424477553793</title>\n",
       "<polygon fill=\"none\" points=\"343.885,-612.678 181.038,-612.678 181.038,-576.678 343.885,-576.678 343.885,-612.678\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"262.462\" y=\"-590.478\">('analyzed_data', 'pool1')</text>\n",
       "</g>\n",
       "<!-- 4308161845663928668&#45;&gt;&#45;1443455424477553793 -->\n",
       "<g class=\"edge\" id=\"edge9\"><title>4308161845663928668-&gt;-1443455424477553793</title>\n",
       "<path d=\"M262.462,-540.944C262.462,-549.823 262.462,-558.572 262.462,-566.313\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"258.962,-566.495 262.462,-576.495 265.962,-566.495 258.962,-566.495\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.visualize(format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('analyzed_data', 'pool1'): 'analyzed_cleaned_data',\n",
       " ('analyzed_data', 'pool2'): 'analyzed_cleaned_data',\n",
       " ('cleaned_data', 'pool1'): 'cleaned_data',\n",
       " ('cleaned_data', 'pool2'): 'cleaned_data',\n",
       " ('data', 'pool1'): 'data',\n",
       " ('data', 'pool2'): 'data'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask.delayed import to_task_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('analyzed_data', 'pool1'): (<function apply>,\n",
       "  <function __main__.analyze_data>,\n",
       "  [],\n",
       "  (dict, [['cleaned_data', ('cleaned_data', 'pool1')], ['option', 1]])),\n",
       " ('analyzed_data', 'pool2'): (<function apply>,\n",
       "  <function __main__.analyze_data>,\n",
       "  [],\n",
       "  (dict, [['cleaned_data', ('cleaned_data', 'pool2')], ['option', 1]])),\n",
       " ('cleaned_data', 'pool1'): (<function apply>,\n",
       "  <function __main__.clean_data>,\n",
       "  [],\n",
       "  (dict, [['data', ('data', 'pool1')]])),\n",
       " ('cleaned_data', 'pool2'): (<function apply>,\n",
       "  <function __main__.clean_data>,\n",
       "  [],\n",
       "  (dict, [['data', ('data', 'pool2')]])),\n",
       " ('data', 'pool1'): (<function __main__.load_data>,),\n",
       " ('data', 'pool2'): (<function __main__.load_data>,)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.dsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('analyzed_data', 'pool1'): (<function apply>,\n",
       "  <function __main__.analyze_data>,\n",
       "  [],\n",
       "  (dict, [['cleaned_data', ('cleaned_data', 'pool1')], ['option', 1]])),\n",
       " ('analyzed_data', 'pool2'): (<function apply>,\n",
       "  <function __main__.analyze_data>,\n",
       "  [],\n",
       "  (dict, [['cleaned_data', ('cleaned_data', 'pool2')], ['option', 1]])),\n",
       " ('cleaned_data', 'pool1'): (<function apply>,\n",
       "  <function __main__.clean_data>,\n",
       "  [],\n",
       "  (dict, [['data', ('data', 'pool1')]])),\n",
       " ('cleaned_data', 'pool2'): (<function apply>,\n",
       "  <function __main__.clean_data>,\n",
       "  [],\n",
       "  (dict, [['data', ('data', 'pool2')]])),\n",
       " ('data', 'pool1'): (<function __main__.load_data>,),\n",
       " ('data', 'pool2'): (<function __main__.load_data>,)}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task, dask = to_task_dask(g.funcs)\n",
    "dict(dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9900"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from distributed import Client\n",
    "from time import sleep\n",
    "import random\n",
    "\n",
    "def inc(x):\n",
    "    sleep(random.random() / 10)\n",
    "    return x + 1\n",
    "\n",
    "def dec(x):\n",
    "    sleep(random.random() / 10)\n",
    "    return x - 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(random.random() / 10)\n",
    "    return x + y\n",
    "\n",
    "\n",
    "client = Client()\n",
    "\n",
    "incs = client.map(inc, range(100))\n",
    "decs = client.map(dec, range(100))\n",
    "adds = client.map(add, incs, decs)\n",
    "total = client.submit(sum, adds)\n",
    "\n",
    "del incs, decs, adds\n",
    "total.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n",
      "distributed.comm.tcp - WARNING - Could not set timeout on TCP stream: [Errno 42] Protocol not available\n"
     ]
    }
   ],
   "source": [
    "global IS_COMPUTED\n",
    "IS_COMPUTED = dict()\n",
    "g = setup_graph(use_cluster=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# persist assert en error because the given collection is not of type\n",
    "# dask.base.Base\n",
    "futures = g.async_run()\n",
    "data = map(lambda x: x.compute(), reversed(futures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"analyzed_cleaned_data_{'kwargs': {}}\",\n",
       " \"analyzed_cleaned_data_{'kwargs': {}}\",\n",
       " \"cleaned_data_{'kwargs': {}}\",\n",
       " \"cleaned_data_{'kwargs': {}}\",\n",
       " \"data_{'kwargs': {}}\",\n",
       " \"data_{'kwargs': {}}\"]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ef9c2b7e2dd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m assert sorted(data) == ['analyzed_data', 'analyzed_data',\n\u001b[0;32m----> 2\u001b[0;31m                         'cleaned_data', 'cleaned_data', 'data', 'data']\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# here I do not know why gather still return delayed objects...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "assert sorted(data) == ['analyzed_data', 'analyzed_data',\n",
    "                        'cleaned_data', 'cleaned_data', 'data', 'data']\n",
    "\n",
    "data = g.client.gather(futures)\n",
    "# here I do not know why gather still return delayed objects...\n",
    "# assert isinstance(data[0], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delayed('getattr-5d4165937d9342af4836dc5ec0e96384')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print futures[0].status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data ...\n",
      "save data with key ('data', 'pool1') ...\n",
      "clean data ...\n",
      "save data with key ('cleaned_data', 'pool1') ...\n",
      "analyze data ...\n",
      "save data with key ('analyzed_data', 'pool1') ...\n",
      "data= analyzed_data\n",
      "Checking that it is cached\n",
      "data= analyzed_data\n",
      "data= cleaned_data\n",
      "\n",
      "load data ...\n",
      "save data with key ('data', 'pool2') ...\n",
      "clean data ...\n",
      "save data with key ('cleaned_data', 'pool2') ...\n",
      "data= cleaned_data\n",
      "\n",
      "analyze data ...\n",
      "save data with key ('analyzed_data', 'pool2') ...\n",
      "data= analyzed_data\n",
      "\n",
      "get multiple results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{('analyzed_data', 'pool1'): 'analyzed_data',\n",
       " ('analyzed_data', 'pool2'): 'analyzed_data'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global variable to simulate the fact to have serialize data somewhere\n",
    "IS_COMPUTED = dict()\n",
    "\n",
    "from functools import partial\n",
    "from dask import get\n",
    "from dask.optimize import cull\n",
    "from functools import wraps\n",
    "import inspect\n",
    "        \n",
    "def load_data():\n",
    "    print 'load data ...'\n",
    "    return 'data'\n",
    "\n",
    "def clean_data(data):\n",
    "    assert isinstance(data, str)\n",
    "    assert data == 'data'\n",
    "    print 'clean data ...'\n",
    "    return 'cleaned_data'\n",
    "\n",
    "def analyze_data(cleaned_data, option=1, **other_options):\n",
    "    assert isinstance(cleaned_data, str)\n",
    "    assert cleaned_data == 'cleaned_data'\n",
    "    print 'analyze data ...'\n",
    "    return 'analyzed_data'\n",
    "\n",
    "class Serializer(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def load(self, key):\n",
    "        print \"load data for key {} ...\".format(key)\n",
    "        return key\n",
    "    \n",
    "    def dump(self, key, value):\n",
    "        print \"save data with key {} ...\".format(key)\n",
    "        IS_COMPUTED[key] = True\n",
    "        \n",
    "    def is_computed(self, key):\n",
    "        return IS_COMPUTED.get(key)\n",
    "    \n",
    "    def delayed_load(self, key):\n",
    "        def load():\n",
    "            return self.load(key)\n",
    "        return load\n",
    "    \n",
    "    def dump_result(self, func, key):\n",
    "        @wraps(func)\n",
    "        def wrapped_func(*args, **kwargs):\n",
    "            result =  func(*args, **kwargs)\n",
    "            self.dump(key, result)\n",
    "            return result\n",
    "        return wrapped_func\n",
    "\n",
    "\n",
    "class Graph(object):\n",
    "    def __init__(self):\n",
    "        self.dsk = dict()\n",
    "        self.cache = dict()\n",
    "        self.serializer = dict()\n",
    "        \n",
    "    def add_task(self, key, serializer, func, *args, **kwargs):\n",
    "        self.serializer[key] = serializer\n",
    "        # prepare arguments for the dask graph specification\n",
    "        args_dict = inspect.getcallargs(func, *args, **kwargs)\n",
    "        args_spec = inspect.getargspec(func)\n",
    "        args_list = [args_dict[argname] for argname in args_spec.args]\n",
    "        # dump data as side effect\n",
    "        func = serializer.dump_result(func, key)\n",
    "        # propagate keyword arguments\n",
    "        if args_spec.keywords:\n",
    "            func = partial(func, **args_dict[args_spec.keywords])\n",
    "        # add task to dask graph\n",
    "        self.dsk[key] = (func,) + tuple(args_list)\n",
    "        return key\n",
    "        \n",
    "    @property\n",
    "    def persistent_dsk(self):\n",
    "        dsk = self.dsk.copy()\n",
    "        # load instead of compute\n",
    "        # the fact to call the method \"is_computed\" may slow down the code.\n",
    "        dsk.update({key : (self.serializer[key].delayed_load(key),) for key in dsk.keys() if key in self.serializer and self.serializer[key].is_computed(key)})\n",
    "        #Use cache instead of loadind\n",
    "        dsk.update({key : self.cache[key] for key in dsk.keys() if key in self.cache})\n",
    "        return dsk\n",
    "        \n",
    "    def run(self, key=None):\n",
    "        if key is None:\n",
    "            key = self.dsk.keys()\n",
    "        dsk = self.persistent_dsk\n",
    "        dsk, _ = cull(dsk, key)\n",
    "        # get all necessary results\n",
    "        keys = dsk.keys()\n",
    "        results = dict(zip(keys, get(dsk, keys)))\n",
    "        # store in cache\n",
    "        self.cache.update(results)\n",
    "        \n",
    "    def get(self, key):\n",
    "        self.run(key)\n",
    "        try:\n",
    "            return self.cache[key]\n",
    "        except (TypeError, KeyError):\n",
    "            return {k : self.cache[k] for k in key}\n",
    "    \n",
    "\n",
    "def setup_graph(**kwargs):\n",
    "    g = Graph()\n",
    "    serializer = Serializer()\n",
    "    for pool in ['pool1', 'pool2']:\n",
    "        g.add_task(('data', pool), serializer, load_data)\n",
    "        g.add_task(('cleaned_data', pool), serializer, clean_data, ('data', pool) )\n",
    "        g.add_task(('analyzed_data', pool), serializer, analyze_data, ('cleaned_data', pool))\n",
    "    return g\n",
    "\n",
    "g = setup_graph()\n",
    "data = g.get(('analyzed_data', 'pool1'))\n",
    "print \"data=\", data\n",
    "print \"Checking that it is cached\"\n",
    "data = g.get(('analyzed_data', 'pool1'))\n",
    "print \"data=\", data\n",
    "data = g.get(('cleaned_data', 'pool1'))\n",
    "print \"data=\", data\n",
    "print \"\"\n",
    "\n",
    "\n",
    "data = g.get(('cleaned_data', 'pool2'))\n",
    "print \"data=\", data\n",
    "print \"\"\n",
    "\n",
    "\n",
    "data = g.get(('analyzed_data', 'pool2'))\n",
    "print \"data=\", data\n",
    "print \"\"\n",
    "\n",
    "print \"get multiple results\"\n",
    "g.get([('analyzed_data', 'pool1'), ('analyzed_data', 'pool2')])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "func = analyze_data\n",
    "\n",
    "args_dict = inspect.getcallargs(func, \"toto\", 2, test=True)\n",
    "args_spec = inspect.getargspec(func)\n",
    "args_list = [args_dict[argname] for argname in args_spec.args]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "\n",
    "IS_COMPUTED = dict()\n",
    "g = setup_graph()\n",
    "# the first time the grap is created it has functions\n",
    "pprint(g.persistent_dsk)\n",
    "# run the graph\n",
    "g.run()\n",
    "# then the graph is replaced by cached data\n",
    "pprint(g.persistent_dsk)\n",
    "\n",
    "# If we recreate a new graph (the cache is delete)\n",
    "g = setup_graph()\n",
    "# the graph conainte the load methods\n",
    "pprint(g.persistent_dsk)\n",
    "\n",
    "\n",
    "print \"get multiple results\"\n",
    "data = g.get([('analyzed_data', 'pool1'), ('analyzed_data', 'pool2')])\n",
    "print \"data=\", data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.base import Base\n",
    "Base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from dask.bag import Bag\n",
    "b =Bag(g.dsk, ('analyzed_data', 'pool2'), None)\n",
    "b.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dask import delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = delayed(load_data)(dask_key_name=('data', 'pool1'))\n",
    "cleaned_data= delayed(clean_data)(dask_key_name=('cleaned_data', 'pool1'), **dict(data=data))\n",
    "cleaned_data.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleaned_data.visualize(format='svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "get(cleaned_data.dask, ('cleaned_data', 'pool1'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
